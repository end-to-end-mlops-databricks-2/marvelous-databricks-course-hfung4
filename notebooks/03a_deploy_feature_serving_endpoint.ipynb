{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import requests\n",
    "from databricks import feature_engineering\n",
    "from pyspark.dbutils import DBUtils\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from airbnb_listing.config import config\n",
    "from airbnb_listing.serving.feature_serving import FeatureServing\n",
    "from airbnb_listing.env import DB_HOST,DB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = feature_engineering.FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"DB_HOST\"] = DB_HOST\n",
    "os.environ[\"DB_TOKEN\"] = DB_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define catalog, schema, and feature table, feature spec, and endpoint names\n",
    "catalog_name = config.general.DEV_CATALOG\n",
    "model_asset_schema_name = config.general.ML_ASSET_SCHEMA\n",
    "feature_table_name = f\"{catalog_name}.{model_asset_schema_name}.airbnb_listing_preds\"\n",
    "feature_spec_name = f\"{catalog_name}.{model_asset_schema_name}.return_predictions\"\n",
    "endpoint_name = \"airbnb-listing--feature-serving\"\n",
    "\n",
    "silver_schema_name = config.general.SILVER_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and test set, and then combine them into a single dataframe\n",
    "train_set = spark.table(f\"{catalog_name}.{silver_schema_name}.airbnb_listing_price_train\").toPandas()\n",
    "test_set = spark.table(f\"{catalog_name}.{silver_schema_name}.airbnb_listing_price_test\").toPandas()\n",
    "full_df = pd.concat([train_set, test_set])\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config.model.MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest basic model\n",
    "model = mlflow.sklearn.load_model(f\"models:/{catalog_name}.{model_asset_schema_name}.{model_name}_basic@latest-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference (pred) table\n",
    "preds_df = full_df[[config.model.ID_COLUMN,\"latitude\", \"longitude\"]].copy()\n",
    "# Add predicted_listing_price to the preds_df by performing inference with full_df and the trained model\n",
    "preds_df[\"predicted_listing_price\"] = model.predict(full_df[config.model.SELECTED_NUMERIC_FEATURES + config.model.SELECTED_CATEGORICAL_FEATURES])\n",
    "# Convert to a spark dataframe\n",
    "preds_df = spark.createDataFrame(preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature table from preds_df (the inference table in spark)\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[config.model.ID_COLUMN],\n",
    "    df=preds_df,\n",
    "    description=\"Airbnb listing prices predictions feature table\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order for the predictions (offline) predictions table to be served, I need to create a\n",
    "# read-copy low-latency copy of it (the online table). If I don't want to copy the full offline\n",
    "# table to the online table at each trigger, I need to enable ChangeDataFeed for the offline feature table\n",
    "spark.sql(f\"\"\"\n",
    "          ALTER TABLE {feature_table_name}\n",
    "          SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature store manager\n",
    "feature_serving = FeatureServing(\n",
    "    feature_table_name=feature_table_name, feature_spec_name=feature_spec_name, endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create online table\n",
    "feature_serving.create_online_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
